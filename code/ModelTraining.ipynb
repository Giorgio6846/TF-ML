{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18accfaa",
   "metadata": {},
   "source": [
    "# Importe de librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3eb361c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91b36912",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import LitPriceData\n",
    "from trainer import LitTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17e50b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import splitData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52704334",
   "metadata": {},
   "source": [
    "# Carga de monedas selecionadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c604b931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "0",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "9160cc16-892b-4ae6-8dc1-6fe9c44025e3",
       "rows": [
        [
         "0",
         "celestia"
        ],
        [
         "1",
         "flare-networks"
        ],
        [
         "2",
         "polygon-bridged-usdt-polygon"
        ],
        [
         "3",
         "xdce-crowd-sale"
        ],
        [
         "4",
         "injective-protocol"
        ],
        [
         "5",
         "virtual-protocol"
        ],
        [
         "6",
         "blockstack"
        ],
        [
         "7",
         "sonic-3"
        ],
        [
         "8",
         "binance-bridged-usdc-bnb-smart-chain"
        ],
        [
         "9",
         "mantle-staked-ether"
        ],
        [
         "10",
         "stakewise-v3-oseth"
        ],
        [
         "11",
         "optimism"
        ],
        [
         "12",
         "pudgy-penguins"
        ],
        [
         "13",
         "syrupusdc"
        ],
        [
         "14",
         "story-2"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 15
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>celestia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>flare-networks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>polygon-bridged-usdt-polygon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>xdce-crowd-sale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>injective-protocol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>virtual-protocol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>blockstack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sonic-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>binance-bridged-usdc-bnb-smart-chain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>mantle-staked-ether</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>stakewise-v3-oseth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>optimism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>pudgy-penguins</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>syrupusdc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>story-2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       0\n",
       "0                               celestia\n",
       "1                         flare-networks\n",
       "2           polygon-bridged-usdt-polygon\n",
       "3                        xdce-crowd-sale\n",
       "4                     injective-protocol\n",
       "5                       virtual-protocol\n",
       "6                             blockstack\n",
       "7                                sonic-3\n",
       "8   binance-bridged-usdc-bnb-smart-chain\n",
       "9                    mantle-staked-ether\n",
       "10                    stakewise-v3-oseth\n",
       "11                              optimism\n",
       "12                        pudgy-penguins\n",
       "13                             syrupusdc\n",
       "14                               story-2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coins = pd.read_json('../data/selected_coins.json')\n",
    "coins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "788753a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(model, X, y, checkpoint_callback=None):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    dataModule = LitPriceData(X_train, y_train, X_val, y_val)\n",
    "       \n",
    "    if checkpoint_callback is not None:\n",
    "        trainer = L.Trainer(max_epochs=75, accelerator=\"auto\", callbacks=[checkpoint_callback])\n",
    "    else:\n",
    "        trainer = L.Trainer(max_epochs=75, accelerator=\"auto\")\n",
    "\n",
    "    trainer.fit(model=model, datamodule=dataModule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fc7b86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/giorgio6846/miniconda3/envs/TF-ML/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "/home/giorgio6846/miniconda3/envs/TF-ML/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:658: Checkpoint directory /home/giorgio6846/Code/Clases/Machine Learning/TF/models/checkpoints/celestia exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type      | Params | Mode \n",
      "--------------------------------------------\n",
      "0 | model | LSTMModel | 72.4 K | train\n",
      "--------------------------------------------\n",
      "72.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "72.4 K    Total params\n",
      "0.290     Total estimated model params size (MB)\n",
      "3         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giorgio6846/miniconda3/envs/TF-ML/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giorgio6846/miniconda3/envs/TF-ML/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/giorgio6846/miniconda3/envs/TF-ML/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74: 100%|██████████| 26/26 [00:00<00:00, 340.28it/s, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=75` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74: 100%|██████████| 26/26 [00:00<00:00, 337.71it/s, v_num=0]\n",
      "Best checkpoint for celestia: /home/giorgio6846/Code/Clases/Machine Learning/TF/models/checkpoints/celestia/checkpoints-v1.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/giorgio6846/miniconda3/envs/TF-ML/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:658: Checkpoint directory /home/giorgio6846/Code/Clases/Machine Learning/TF/models/checkpoints/flare-networks exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type      | Params | Mode \n",
      "--------------------------------------------\n",
      "0 | model | LSTMModel | 72.4 K | train\n",
      "--------------------------------------------\n",
      "72.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "72.4 K    Total params\n",
      "0.290     Total estimated model params size (MB)\n",
      "3         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giorgio6846/miniconda3/envs/TF-ML/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/giorgio6846/miniconda3/envs/TF-ML/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/giorgio6846/miniconda3/envs/TF-ML/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74: 100%|██████████| 26/26 [00:00<00:00, 331.55it/s, v_num=1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=75` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74: 100%|██████████| 26/26 [00:00<00:00, 328.96it/s, v_num=1]\n",
      "Best checkpoint for flare-networks: /home/giorgio6846/Code/Clases/Machine Learning/TF/models/checkpoints/flare-networks/checkpoints-v1.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/giorgio6846/miniconda3/envs/TF-ML/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:658: Checkpoint directory /home/giorgio6846/Code/Clases/Machine Learning/TF/models/checkpoints/polygon-bridged-usdt-polygon exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type      | Params | Mode \n",
      "--------------------------------------------\n",
      "0 | model | LSTMModel | 72.4 K | train\n",
      "--------------------------------------------\n",
      "72.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "72.4 K    Total params\n",
      "0.290     Total estimated model params size (MB)\n",
      "3         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giorgio6846/miniconda3/envs/TF-ML/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/giorgio6846/miniconda3/envs/TF-ML/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/giorgio6846/miniconda3/envs/TF-ML/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74: 100%|██████████| 26/26 [00:00<00:00, 317.72it/s, v_num=2]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=75` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74: 100%|██████████| 26/26 [00:00<00:00, 314.26it/s, v_num=2]\n",
      "Best checkpoint for polygon-bridged-usdt-polygon: /home/giorgio6846/Code/Clases/Machine Learning/TF/models/checkpoints/polygon-bridged-usdt-polygon/checkpoints-v1.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/giorgio6846/miniconda3/envs/TF-ML/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:658: Checkpoint directory /home/giorgio6846/Code/Clases/Machine Learning/TF/models/checkpoints/xdce-crowd-sale exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type      | Params | Mode \n",
      "--------------------------------------------\n",
      "0 | model | LSTMModel | 72.4 K | train\n",
      "--------------------------------------------\n",
      "72.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "72.4 K    Total params\n",
      "0.290     Total estimated model params size (MB)\n",
      "3         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giorgio6846/miniconda3/envs/TF-ML/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/giorgio6846/miniconda3/envs/TF-ML/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/giorgio6846/miniconda3/envs/TF-ML/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74: 100%|██████████| 26/26 [00:00<00:00, 326.93it/s, v_num=3]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=75` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74: 100%|██████████| 26/26 [00:00<00:00, 323.82it/s, v_num=3]\n",
      "Best checkpoint for xdce-crowd-sale: /home/giorgio6846/Code/Clases/Machine Learning/TF/models/checkpoints/xdce-crowd-sale/checkpoints-v1.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/giorgio6846/miniconda3/envs/TF-ML/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:658: Checkpoint directory /home/giorgio6846/Code/Clases/Machine Learning/TF/models/checkpoints/injective-protocol exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type      | Params | Mode \n",
      "--------------------------------------------\n",
      "0 | model | LSTMModel | 72.4 K | train\n",
      "--------------------------------------------\n",
      "72.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "72.4 K    Total params\n",
      "0.290     Total estimated model params size (MB)\n",
      "3         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giorgio6846/miniconda3/envs/TF-ML/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/giorgio6846/miniconda3/envs/TF-ML/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/giorgio6846/miniconda3/envs/TF-ML/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74: 100%|██████████| 26/26 [00:00<00:00, 339.09it/s, v_num=4]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=75` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74: 100%|██████████| 26/26 [00:00<00:00, 335.53it/s, v_num=4]\n",
      "Best checkpoint for injective-protocol: /home/giorgio6846/Code/Clases/Machine Learning/TF/models/checkpoints/injective-protocol/checkpoints-v1.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/giorgio6846/miniconda3/envs/TF-ML/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:658: Checkpoint directory /home/giorgio6846/Code/Clases/Machine Learning/TF/models/checkpoints/virtual-protocol exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type      | Params | Mode \n",
      "--------------------------------------------\n",
      "0 | model | LSTMModel | 72.4 K | train\n",
      "--------------------------------------------\n",
      "72.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "72.4 K    Total params\n",
      "0.290     Total estimated model params size (MB)\n",
      "3         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giorgio6846/miniconda3/envs/TF-ML/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/giorgio6846/miniconda3/envs/TF-ML/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/giorgio6846/miniconda3/envs/TF-ML/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74: 100%|██████████| 26/26 [00:00<00:00, 386.22it/s, v_num=5]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=75` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74: 100%|██████████| 26/26 [00:00<00:00, 381.95it/s, v_num=5]\n",
      "Best checkpoint for virtual-protocol: /home/giorgio6846/Code/Clases/Machine Learning/TF/models/checkpoints/virtual-protocol/checkpoints-v1.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/giorgio6846/miniconda3/envs/TF-ML/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:658: Checkpoint directory /home/giorgio6846/Code/Clases/Machine Learning/TF/models/checkpoints/blockstack exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type      | Params | Mode \n",
      "--------------------------------------------\n",
      "0 | model | LSTMModel | 72.4 K | train\n",
      "--------------------------------------------\n",
      "72.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "72.4 K    Total params\n",
      "0.290     Total estimated model params size (MB)\n",
      "3         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giorgio6846/miniconda3/envs/TF-ML/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/giorgio6846/miniconda3/envs/TF-ML/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/giorgio6846/miniconda3/envs/TF-ML/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74: 100%|██████████| 26/26 [00:00<00:00, 337.68it/s, v_num=6]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=75` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74: 100%|██████████| 26/26 [00:00<00:00, 334.72it/s, v_num=6]\n",
      "Best checkpoint for blockstack: /home/giorgio6846/Code/Clases/Machine Learning/TF/models/checkpoints/blockstack/checkpoints-v1.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/giorgio6846/miniconda3/envs/TF-ML/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:658: Checkpoint directory /home/giorgio6846/Code/Clases/Machine Learning/TF/models/checkpoints/sonic-3 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type      | Params | Mode \n",
      "--------------------------------------------\n",
      "0 | model | LSTMModel | 72.4 K | train\n",
      "--------------------------------------------\n",
      "72.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "72.4 K    Total params\n",
      "0.290     Total estimated model params size (MB)\n",
      "3         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giorgio6846/miniconda3/envs/TF-ML/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/giorgio6846/miniconda3/envs/TF-ML/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/giorgio6846/miniconda3/envs/TF-ML/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74: 100%|██████████| 26/26 [00:00<00:00, 413.31it/s, v_num=7]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=75` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74: 100%|██████████| 26/26 [00:00<00:00, 409.14it/s, v_num=7]\n",
      "Best checkpoint for sonic-3: /home/giorgio6846/Code/Clases/Machine Learning/TF/models/checkpoints/sonic-3/checkpoints-v1.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/giorgio6846/miniconda3/envs/TF-ML/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:658: Checkpoint directory /home/giorgio6846/Code/Clases/Machine Learning/TF/models/checkpoints/binance-bridged-usdc-bnb-smart-chain exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type      | Params | Mode \n",
      "--------------------------------------------\n",
      "0 | model | LSTMModel | 72.4 K | train\n",
      "--------------------------------------------\n",
      "72.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "72.4 K    Total params\n",
      "0.290     Total estimated model params size (MB)\n",
      "3         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giorgio6846/miniconda3/envs/TF-ML/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/giorgio6846/miniconda3/envs/TF-ML/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/giorgio6846/miniconda3/envs/TF-ML/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74: 100%|██████████| 26/26 [00:00<00:00, 340.87it/s, v_num=8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=75` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74: 100%|██████████| 26/26 [00:00<00:00, 338.39it/s, v_num=8]\n",
      "Best checkpoint for binance-bridged-usdc-bnb-smart-chain: /home/giorgio6846/Code/Clases/Machine Learning/TF/models/checkpoints/binance-bridged-usdc-bnb-smart-chain/checkpoints-v1.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/giorgio6846/miniconda3/envs/TF-ML/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:658: Checkpoint directory /home/giorgio6846/Code/Clases/Machine Learning/TF/models/checkpoints/mantle-staked-ether exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type      | Params | Mode \n",
      "--------------------------------------------\n",
      "0 | model | LSTMModel | 72.4 K | train\n",
      "--------------------------------------------\n",
      "72.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "72.4 K    Total params\n",
      "0.290     Total estimated model params size (MB)\n",
      "3         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giorgio6846/miniconda3/envs/TF-ML/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/giorgio6846/miniconda3/envs/TF-ML/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/giorgio6846/miniconda3/envs/TF-ML/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74: 100%|██████████| 26/26 [00:00<00:00, 332.40it/s, v_num=9]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=75` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74: 100%|██████████| 26/26 [00:00<00:00, 328.94it/s, v_num=9]\n",
      "Best checkpoint for mantle-staked-ether: /home/giorgio6846/Code/Clases/Machine Learning/TF/models/checkpoints/mantle-staked-ether/checkpoints-v1.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/giorgio6846/miniconda3/envs/TF-ML/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:658: Checkpoint directory /home/giorgio6846/Code/Clases/Machine Learning/TF/models/checkpoints/stakewise-v3-oseth exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type      | Params | Mode \n",
      "--------------------------------------------\n",
      "0 | model | LSTMModel | 72.4 K | train\n",
      "--------------------------------------------\n",
      "72.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "72.4 K    Total params\n",
      "0.290     Total estimated model params size (MB)\n",
      "3         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giorgio6846/miniconda3/envs/TF-ML/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/giorgio6846/miniconda3/envs/TF-ML/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/giorgio6846/miniconda3/envs/TF-ML/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (25) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74: 100%|██████████| 25/25 [00:00<00:00, 327.59it/s, v_num=10]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=75` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74: 100%|██████████| 25/25 [00:00<00:00, 323.84it/s, v_num=10]\n",
      "Best checkpoint for stakewise-v3-oseth: /home/giorgio6846/Code/Clases/Machine Learning/TF/models/checkpoints/stakewise-v3-oseth/checkpoints-v1.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/giorgio6846/miniconda3/envs/TF-ML/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:658: Checkpoint directory /home/giorgio6846/Code/Clases/Machine Learning/TF/models/checkpoints/optimism exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type      | Params | Mode \n",
      "--------------------------------------------\n",
      "0 | model | LSTMModel | 72.4 K | train\n",
      "--------------------------------------------\n",
      "72.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "72.4 K    Total params\n",
      "0.290     Total estimated model params size (MB)\n",
      "3         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giorgio6846/miniconda3/envs/TF-ML/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/giorgio6846/miniconda3/envs/TF-ML/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/giorgio6846/miniconda3/envs/TF-ML/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74: 100%|██████████| 26/26 [00:00<00:00, 382.17it/s, v_num=11]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=75` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74: 100%|██████████| 26/26 [00:00<00:00, 378.77it/s, v_num=11]\n",
      "Best checkpoint for optimism: /home/giorgio6846/Code/Clases/Machine Learning/TF/models/checkpoints/optimism/checkpoints-v1.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/giorgio6846/miniconda3/envs/TF-ML/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:658: Checkpoint directory /home/giorgio6846/Code/Clases/Machine Learning/TF/models/checkpoints/pudgy-penguins exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type      | Params | Mode \n",
      "--------------------------------------------\n",
      "0 | model | LSTMModel | 72.4 K | train\n",
      "--------------------------------------------\n",
      "72.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "72.4 K    Total params\n",
      "0.290     Total estimated model params size (MB)\n",
      "3         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giorgio6846/miniconda3/envs/TF-ML/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/giorgio6846/miniconda3/envs/TF-ML/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/giorgio6846/miniconda3/envs/TF-ML/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74: 100%|██████████| 26/26 [00:00<00:00, 392.06it/s, v_num=12]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=75` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74: 100%|██████████| 26/26 [00:00<00:00, 387.81it/s, v_num=12]\n",
      "Best checkpoint for pudgy-penguins: /home/giorgio6846/Code/Clases/Machine Learning/TF/models/checkpoints/pudgy-penguins/checkpoints-v1.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/giorgio6846/miniconda3/envs/TF-ML/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:658: Checkpoint directory /home/giorgio6846/Code/Clases/Machine Learning/TF/models/checkpoints/syrupusdc exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type      | Params | Mode \n",
      "--------------------------------------------\n",
      "0 | model | LSTMModel | 72.4 K | train\n",
      "--------------------------------------------\n",
      "72.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "72.4 K    Total params\n",
      "0.290     Total estimated model params size (MB)\n",
      "3         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giorgio6846/miniconda3/envs/TF-ML/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/giorgio6846/miniconda3/envs/TF-ML/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/giorgio6846/miniconda3/envs/TF-ML/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74: 100%|██████████| 26/26 [00:00<00:00, 418.95it/s, v_num=13]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=75` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74: 100%|██████████| 26/26 [00:00<00:00, 414.54it/s, v_num=13]\n",
      "Best checkpoint for syrupusdc: /home/giorgio6846/Code/Clases/Machine Learning/TF/models/checkpoints/syrupusdc/checkpoints-v1.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/giorgio6846/miniconda3/envs/TF-ML/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:658: Checkpoint directory /home/giorgio6846/Code/Clases/Machine Learning/TF/models/checkpoints/story-2 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type      | Params | Mode \n",
      "--------------------------------------------\n",
      "0 | model | LSTMModel | 72.4 K | train\n",
      "--------------------------------------------\n",
      "72.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "72.4 K    Total params\n",
      "0.290     Total estimated model params size (MB)\n",
      "3         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giorgio6846/miniconda3/envs/TF-ML/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/giorgio6846/miniconda3/envs/TF-ML/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/giorgio6846/miniconda3/envs/TF-ML/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74: 100%|██████████| 26/26 [00:00<00:00, 307.40it/s, v_num=14]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=75` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74: 100%|██████████| 26/26 [00:00<00:00, 294.27it/s, v_num=14]\n",
      "Best checkpoint for story-2: /home/giorgio6846/Code/Clases/Machine Learning/TF/models/checkpoints/story-2/checkpoints-v1.ckpt\n"
     ]
    }
   ],
   "source": [
    "hyperparams = {'hidden_size': 76,\n",
    " 'num_layers': 2,\n",
    " 'lr': 0.005094100053762949,\n",
    " 'dropout': 0.4197932446614159,\n",
    " 'sequence_legth': 12}\n",
    "\n",
    "for i, row in coins.iterrows():    \n",
    "    name = row[0]\n",
    "    export_dir = f\"../models/exports/{name}\"\n",
    "    os.makedirs(export_dir, exist_ok=True)\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath = f\"../models/checkpoints/{name}\",\n",
    "        filename = \"checkpoints\",\n",
    "        save_top_k=1,\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\"\n",
    "    )\n",
    "\n",
    "    #Change the values\n",
    "    model = LitTrainer(hidden_size=hyperparams[\"hidden_size\"], num_layers=hyperparams[\"num_layers\"], lr=hyperparams[\"lr\"], dropout=hyperparams[\"dropout\"])\n",
    "    X, y = splitData(name, sequence_length=hyperparams[\"sequence_legth\"])\n",
    "    trainModel(model, X, y, checkpoint_callback)\n",
    "    \n",
    "    best_ckpt_path = checkpoint_callback.best_model_path\n",
    "    print(f\"Best checkpoint for {name}: {best_ckpt_path}\")\n",
    "\n",
    "    best_model = LitTrainer.load_from_checkpoint(best_ckpt_path, hidden_size=hyperparams[\"hidden_size\"], num_layers=hyperparams[\"num_layers\"], dropout=hyperparams[\"dropout\"])\n",
    "\n",
    "    export_state_dict_path = os.path.join(export_dir, \"best_model.pth\")\n",
    "    torch.save(best_model.state_dict(), export_state_dict_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF-ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
